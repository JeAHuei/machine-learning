{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Q-learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-table\n",
    "\n",
    "In the case of FrozenLake game, we have 16 * 4 table of Q-values. We start by initializing the table to be uniform(all zeros).\n",
    "\n",
    "    Q = np.zeros([env.observation_space.n,env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Environment\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0') #Load environment\n",
    "print (\"Agent Environment\")\n",
    "env.render()  # Output 4*4 state\n",
    "rList = [] # Record reward\n",
    "lr = .85\n",
    "y = .99     # weight\n",
    "num_episodes = 2000  #training times\n",
    "Q = np.zeros([env.observation_space.n,env.action_space.n]) # Initialize Q-table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Q-table to make decisions `test`\n",
    "\n",
    "* set record times(such as 4) to check move times to goals in every episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(i):\n",
    "    d1 = False\n",
    "    j1 = 0\n",
    "    start = 0\n",
    "    r_sum = 0\n",
    "    while d1 == False:  \n",
    "        j1 +=1\n",
    "        a = np.argmax(Q[start,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        s1,r,d1,_ = env.step(a)\n",
    "        start = s1\n",
    "        r_sum +=r\n",
    "    if(r_sum == 1.0):\n",
    "        print ('To Goal -- Times',j1)\n",
    "    else:\n",
    "        print ('Not to Goal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning Algorithm\n",
    "\n",
    "    Initialize Q(s,a) arbitrarily\n",
    "    Repeat (for each episoda):\n",
    "        Initialize s\n",
    "        Repeat(for each step of episode):\n",
    "            Choose a from s using policy derived from Q(e.g., epsilon-greedy)\n",
    "            Take action a, observe r, s'\n",
    "            Q(s,a) <—— Q(s,a) + alpha[reward + gamma * maxQ(s',a') - Q(s,a)]\n",
    "            s <—— s'\n",
    "        until s is terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning():\n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()  # Reset environment\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "\n",
    "        if (i % 500) ==0:  # test Q-table\n",
    "            test(i)\n",
    "\n",
    "        while j < 99:  # Q-learning Algorithm\n",
    "            j+=1\n",
    "            a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1))) # Greedy action\n",
    "            s1,r,d,_ = env.step(a)  # Obtain new state and new reward\n",
    "            Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])  # Update Q-table\n",
    "            rAll += r\n",
    "            s = s1 # Update state\n",
    "            if d == True:  # If agent go to the goals, break\n",
    "                break\n",
    "\n",
    "        rList.append(rAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not to Goal\n",
      "To Goal -- Times 45\n",
      "To Goal -- Times 93\n",
      "Not to Goal\n",
      "Accurately: 42.35%\n",
      "Q-Table\n",
      "[[  6.94393311e-03   9.22273669e-03   7.98197631e-01   1.52587948e-02]\n",
      " [  2.54983908e-04   2.40952682e-04   1.70217715e-03   4.79265981e-01]\n",
      " [  1.87617220e-03   6.92227004e-03   2.88978635e-03   2.96276911e-01]\n",
      " [  6.74502081e-04   1.70874729e-03   4.01216360e-05   1.97994534e-01]\n",
      " [  8.81168022e-01   9.52888495e-04   7.63777537e-04   8.19136195e-04]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  4.14501407e-02   3.61209013e-04   6.48114295e-04   6.50031736e-05]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  5.96933584e-04   5.09564402e-06   2.27069141e-04   9.15041882e-01]\n",
      " [  0.00000000e+00   3.72380812e-01   1.36572750e-04   5.97140490e-04]\n",
      " [  8.81861318e-01   7.06644577e-04   0.00000000e+00   5.61846973e-04]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  7.04790792e-05   4.28103812e-04   9.73697905e-01   6.24231220e-05]\n",
      " [  3.94679883e-03   9.98518477e-01   0.00000000e+00   4.03450101e-03]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "Q_learning()\n",
    "print (\"Accurately: \" +  str(sum(rList)/num_episodes*100) + \"%\")\n",
    "print (\"Q-Table\")\n",
    "print (Q) # Output Q-Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Training for 2000 times: record Intermediate results。\n",
    "\n",
    "We found that the probability of finding Object is gradually increase.\n",
    "\n",
    "At the same time, we found that Q-table is hard to extend, because  the states of real world or other games maybe too big to describe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
